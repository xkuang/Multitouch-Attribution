{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6481f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pymc3\n",
    "#!pip install pymc\n",
    "#!pip install hmmlearn gensim\n",
    "#!pip install econml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4911b05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points: 3024\n",
      "   user_id touchpoint  timestamp  conversion\n",
      "0        1      video 2023-03-29           0\n",
      "1        1     social 2023-04-10           0\n",
      "2        1     search 2023-08-03           0\n",
      "3        1      video 2023-11-27           0\n",
      "4        1     search 2023-12-26           0\n",
      "5        2  affiliate 2023-07-11           0\n",
      "6        2    display 2023-10-21           0\n",
      "7        3  affiliate 2023-02-18           0\n",
      "8        3      video 2023-02-28           0\n",
      "9        3    display 2023-06-19           1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Function to generate random touchpoint sequences\n",
    "def generate_touchpoint_sequence():\n",
    "    touchpoints = ['email', 'social', 'search', 'display', 'video', 'affiliate']\n",
    "    sequence_length = np.random.randint(1, 6)\n",
    "    return np.random.choice(touchpoints, size=sequence_length, replace=True)\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "n_users = 1000\n",
    "max_touchpoints_per_user = 5\n",
    "\n",
    "data = []\n",
    "start_date = datetime(2023, 1, 1)\n",
    "\n",
    "for user_id in range(1, n_users + 1):\n",
    "    n_touchpoints = np.random.randint(1, max_touchpoints_per_user + 1)\n",
    "    touchpoints = generate_touchpoint_sequence()\n",
    "    \n",
    "    for i, touchpoint in enumerate(touchpoints):\n",
    "        timestamp = start_date + timedelta(days=np.random.randint(0, 365))\n",
    "        conversion = 1 if i == len(touchpoints) - 1 and np.random.random() < 0.2 else 0\n",
    "        data.append([user_id, touchpoint, timestamp, conversion])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['user_id', 'touchpoint', 'timestamp', 'conversion'])\n",
    "df = df.sort_values(['user_id', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total number of data points: {len(df)}\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "244646a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First-Touch Attribution:\n",
      "{'display': 0.183, 'video': 0.181, 'affiliate': 0.166, 'social': 0.16, 'email': 0.159, 'search': 0.151}\n",
      "\n",
      "Last-Touch Attribution:\n",
      "{'search': 0.182, 'email': 0.176, 'social': 0.172, 'display': 0.171, 'video': 0.159, 'affiliate': 0.14}\n",
      "\n",
      "Linear Attribution:\n",
      "{'video': 0.17592592592592593, 'display': 0.16832010582010581, 'search': 0.167989417989418, 'email': 0.16567460317460317, 'social': 0.16468253968253968, 'affiliate': 0.1574074074074074}\n",
      "\n",
      "Comparison of Attributions:\n",
      "Channel    First-Touch     Last-Touch      Linear         \n",
      "-------------------------------------------------------\n",
      "affiliate  0.1660          0.1400          0.1574         \n",
      "display    0.1830          0.1710          0.1683         \n",
      "search     0.1510          0.1820          0.1680         \n",
      "social     0.1600          0.1720          0.1647         \n",
      "video      0.1810          0.1590          0.1759         \n",
      "email      0.1590          0.1760          0.1657         \n",
      "\n",
      "Conversion Analysis:\n",
      "Total conversions: 217\n",
      "\n",
      "Conversions by channel (Last-Touch):\n",
      "video        0.225806\n",
      "social       0.188940\n",
      "search       0.175115\n",
      "email        0.152074\n",
      "affiliate    0.138249\n",
      "display      0.119816\n",
      "Name: touchpoint, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Helper function to calculate attribution\n",
    "def calculate_attribution(attribution_dict):\n",
    "    total = sum(attribution_dict.values())\n",
    "    return {channel: value / total for channel, value in attribution_dict.items()}\n",
    "\n",
    "# 1. First-Touch Attribution Model\n",
    "def first_touch_attribution(df):\n",
    "    first_touches = df.groupby('user_id').first()['touchpoint'].value_counts()\n",
    "    return calculate_attribution(first_touches.to_dict())\n",
    "\n",
    "# 2. Last-Touch Attribution Model\n",
    "def last_touch_attribution(df):\n",
    "    last_touches = df.groupby('user_id').last()['touchpoint'].value_counts()\n",
    "    return calculate_attribution(last_touches.to_dict())\n",
    "\n",
    "# 3. Linear Attribution Model\n",
    "def linear_attribution(df):\n",
    "    all_touches = df['touchpoint'].value_counts()\n",
    "    return calculate_attribution(all_touches.to_dict())\n",
    "\n",
    "# Calculate attributions\n",
    "first_touch_attr = first_touch_attribution(df)\n",
    "last_touch_attr = last_touch_attribution(df)\n",
    "linear_attr = linear_attribution(df)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nFirst-Touch Attribution:\")\n",
    "print(first_touch_attr)\n",
    "\n",
    "print(\"\\nLast-Touch Attribution:\")\n",
    "print(last_touch_attr)\n",
    "\n",
    "print(\"\\nLinear Attribution:\")\n",
    "print(linear_attr)\n",
    "\n",
    "# Comparison of attributions\n",
    "print(\"\\nComparison of Attributions:\")\n",
    "all_channels = set(first_touch_attr.keys()) | set(last_touch_attr.keys()) | set(linear_attr.keys())\n",
    "\n",
    "print(f\"{'Channel':<10} {'First-Touch':<15} {'Last-Touch':<15} {'Linear':<15}\")\n",
    "print(\"-\" * 55)\n",
    "for channel in all_channels:\n",
    "    first_value = first_touch_attr.get(channel, 0)\n",
    "    last_value = last_touch_attr.get(channel, 0)\n",
    "    linear_value = linear_attr.get(channel, 0)\n",
    "    print(f\"{channel:<10} {first_value:<15.4f} {last_value:<15.4f} {linear_value:<15.4f}\")\n",
    "\n",
    "# Conversion analysis\n",
    "print(\"\\nConversion Analysis:\")\n",
    "conversions = df[df['conversion'] == 1]\n",
    "print(f\"Total conversions: {len(conversions)}\")\n",
    "print(\"\\nConversions by channel (Last-Touch):\")\n",
    "print(conversions['touchpoint'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eee24bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "U-shaped Attribution:\n",
      "{'video': 0.1728767693588675, 'search': 0.16319733555370539, 'social': 0.16656258673327784, 'affiliate': 0.15233832917013607, 'display': 0.17703996669442118, 'email': 0.16798501248959208}\n",
      "\n",
      "W-shaped Attribution:\n",
      "{'video': 0.17322794492605834, 'search': 0.16195818459969394, 'social': 0.16715961244263117, 'affiliate': 0.1518612952575213, 'display': 0.17812340642529348, 'email': 0.16766955634880182}\n",
      "\n",
      "Comparison of Attributions:\n",
      "Channel    U-shaped        W-shaped       \n",
      "----------------------------------------\n",
      "affiliate  0.1523          0.1519         \n",
      "display    0.1770          0.1781         \n",
      "search     0.1632          0.1620         \n",
      "social     0.1666          0.1672         \n",
      "video      0.1729          0.1732         \n",
      "email      0.1680          0.1677         \n",
      "\n",
      "Conversion Analysis:\n",
      "Total conversions: 217\n",
      "\n",
      "Conversions by channel (Last-Touch):\n",
      "video        0.225806\n",
      "social       0.188940\n",
      "search       0.175115\n",
      "email        0.152074\n",
      "affiliate    0.138249\n",
      "display      0.119816\n",
      "Name: touchpoint, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Helper function to calculate attribution\n",
    "def calculate_attribution(attribution_dict):\n",
    "    total = sum(attribution_dict.values())\n",
    "    return {channel: value / total for channel, value in attribution_dict.items()}\n",
    "\n",
    "# U-shaped Attribution Model\n",
    "def u_shaped_attribution(df):\n",
    "    attribution = {}\n",
    "    for _, user_journey in df.groupby('user_id'):\n",
    "        journey_length = len(user_journey)\n",
    "        if journey_length == 1:\n",
    "            touchpoint = user_journey['touchpoint'].iloc[0]\n",
    "            attribution[touchpoint] = attribution.get(touchpoint, 0) + 1\n",
    "        else:\n",
    "            first_touch = user_journey['touchpoint'].iloc[0]\n",
    "            last_touch = user_journey['touchpoint'].iloc[-1]\n",
    "            attribution[first_touch] = attribution.get(first_touch, 0) + 0.4\n",
    "            attribution[last_touch] = attribution.get(last_touch, 0) + 0.4\n",
    "            \n",
    "            middle_weight = 0.2 / (journey_length - 2) if journey_length > 2 else 0\n",
    "            for touchpoint in user_journey['touchpoint'].iloc[1:-1]:\n",
    "                attribution[touchpoint] = attribution.get(touchpoint, 0) + middle_weight\n",
    "    \n",
    "    return calculate_attribution(attribution)\n",
    "\n",
    "# W-shaped Attribution Model\n",
    "def w_shaped_attribution(df):\n",
    "    attribution = {}\n",
    "    for _, user_journey in df.groupby('user_id'):\n",
    "        journey_length = len(user_journey)\n",
    "        if journey_length == 1:\n",
    "            touchpoint = user_journey['touchpoint'].iloc[0]\n",
    "            attribution[touchpoint] = attribution.get(touchpoint, 0) + 1\n",
    "        elif journey_length == 2:\n",
    "            first_touch = user_journey['touchpoint'].iloc[0]\n",
    "            last_touch = user_journey['touchpoint'].iloc[-1]\n",
    "            attribution[first_touch] = attribution.get(first_touch, 0) + 0.5\n",
    "            attribution[last_touch] = attribution.get(last_touch, 0) + 0.5\n",
    "        else:\n",
    "            first_touch = user_journey['touchpoint'].iloc[0]\n",
    "            middle_touch = user_journey['touchpoint'].iloc[len(user_journey) // 2]\n",
    "            last_touch = user_journey['touchpoint'].iloc[-1]\n",
    "            attribution[first_touch] = attribution.get(first_touch, 0) + 0.3\n",
    "            attribution[middle_touch] = attribution.get(middle_touch, 0) + 0.3\n",
    "            attribution[last_touch] = attribution.get(last_touch, 0) + 0.3\n",
    "            \n",
    "            other_weight = 0.1 / (journey_length - 3) if journey_length > 3 else 0\n",
    "            for i, touchpoint in enumerate(user_journey['touchpoint']):\n",
    "                if i not in [0, len(user_journey) // 2, len(user_journey) - 1]:\n",
    "                    attribution[touchpoint] = attribution.get(touchpoint, 0) + other_weight\n",
    "    \n",
    "    return calculate_attribution(attribution)\n",
    "\n",
    "# Calculate attributions\n",
    "u_shaped_attr = u_shaped_attribution(df)\n",
    "w_shaped_attr = w_shaped_attribution(df)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nU-shaped Attribution:\")\n",
    "print(u_shaped_attr)\n",
    "\n",
    "print(\"\\nW-shaped Attribution:\")\n",
    "print(w_shaped_attr)\n",
    "\n",
    "# Comparison of attributions\n",
    "print(\"\\nComparison of Attributions:\")\n",
    "all_channels = set(u_shaped_attr.keys()) | set(w_shaped_attr.keys())\n",
    "\n",
    "print(f\"{'Channel':<10} {'U-shaped':<15} {'W-shaped':<15}\")\n",
    "print(\"-\" * 40)\n",
    "for channel in all_channels:\n",
    "    u_value = u_shaped_attr.get(channel, 0)\n",
    "    w_value = w_shaped_attr.get(channel, 0)\n",
    "    print(f\"{channel:<10} {u_value:<15.4f} {w_value:<15.4f}\")\n",
    "\n",
    "# Conversion analysis\n",
    "print(\"\\nConversion Analysis:\")\n",
    "conversions = df[df['conversion'] == 1]\n",
    "print(f\"Total conversions: {len(conversions)}\")\n",
    "print(\"\\nConversions by channel (Last-Touch):\")\n",
    "print(conversions['touchpoint'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d776290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data-Driven Attribution:\n",
      "email: 0.1062\n",
      "social: 0.1946\n",
      "search: 0.1927\n",
      "display: 0.2067\n",
      "video: 0.1267\n",
      "affiliate: 0.1732\n",
      "\n",
      "Model Accuracy: 0.8950\n",
      "\n",
      "Comparison of Attributions:\n",
      "Channel    Data-Driven    \n",
      "-------------------------\n",
      "affiliate  0.1732         \n",
      "search     0.1927         \n",
      "display    0.2067         \n",
      "social     0.1946         \n",
      "video      0.1267         \n",
      "email      0.1062         \n",
      "\n",
      "Channel Importance Ranking:\n",
      "display: 0.2067\n",
      "social: 0.1946\n",
      "search: 0.1927\n",
      "affiliate: 0.1732\n",
      "video: 0.1267\n",
      "email: 0.1062\n",
      "\n",
      "Conversion Analysis:\n",
      "Total conversions: 217\n",
      "\n",
      "Conversions by channel (Last-Touch):\n",
      "video        0.225806\n",
      "social       0.188940\n",
      "search       0.175115\n",
      "email        0.152074\n",
      "affiliate    0.138249\n",
      "display      0.119816\n",
      "Name: touchpoint, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare data for logistic regression\n",
    "def prepare_data(df):\n",
    "    # Create a feature matrix for each user journey\n",
    "    user_journeys = df.groupby('user_id')['touchpoint'].apply(list).reset_index()\n",
    "    user_journeys['conversion'] = df.groupby('user_id')['conversion'].last().values\n",
    "    \n",
    "    # One-hot encode the touchpoints\n",
    "    touchpoints = ['email', 'social', 'search', 'display', 'video', 'affiliate']\n",
    "    feature_matrix = np.zeros((len(user_journeys), len(touchpoints)))\n",
    "    \n",
    "    for idx, journey in enumerate(user_journeys['touchpoint']):\n",
    "        for touchpoint in journey:\n",
    "            feature_matrix[idx, touchpoints.index(touchpoint)] = 1\n",
    "    \n",
    "    return feature_matrix, user_journeys['conversion'].values\n",
    "\n",
    "# Data-Driven Attribution Model\n",
    "def data_driven_attribution(df):\n",
    "    X, y = prepare_data(df)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train logistic regression model\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get feature importance (coefficients)\n",
    "    importance = model.coef_[0]\n",
    "    \n",
    "    # Create attribution dictionary\n",
    "    touchpoints = ['email', 'social', 'search', 'display', 'video', 'affiliate']\n",
    "    attribution = dict(zip(touchpoints, importance))\n",
    "    \n",
    "    # Normalize attribution scores\n",
    "    total = sum(abs(value) for value in attribution.values())\n",
    "    attribution = {channel: abs(value) / total for channel, value in attribution.items()}\n",
    "    \n",
    "    return attribution, model.score(X_test, y_test)\n",
    "\n",
    "# Calculate data-driven attribution\n",
    "data_driven_attr, model_accuracy = data_driven_attribution(df)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nData-Driven Attribution:\")\n",
    "for channel, score in data_driven_attr.items():\n",
    "    print(f\"{channel}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nModel Accuracy: {model_accuracy:.4f}\")\n",
    "\n",
    "# Comparison with other models (assuming you've calculated these in previous steps)\n",
    "# If not, you can remove or comment out this part\n",
    "print(\"\\nComparison of Attributions:\")\n",
    "all_channels = set(data_driven_attr.keys())\n",
    "\n",
    "print(f\"{'Channel':<10} {'Data-Driven':<15}\")\n",
    "print(\"-\" * 25)\n",
    "for channel in all_channels:\n",
    "    dd_value = data_driven_attr.get(channel, 0)\n",
    "    print(f\"{channel:<10} {dd_value:<15.4f}\")\n",
    "\n",
    "# Additional analysis\n",
    "print(\"\\nChannel Importance Ranking:\")\n",
    "sorted_channels = sorted(data_driven_attr.items(), key=lambda x: x[1], reverse=True)\n",
    "for channel, importance in sorted_channels:\n",
    "    print(f\"{channel}: {importance:.4f}\")\n",
    "\n",
    "# Conversion analysis\n",
    "print(\"\\nConversion Analysis:\")\n",
    "conversions = df[df['conversion'] == 1]\n",
    "print(f\"Total conversions: {len(conversions)}\")\n",
    "print(\"\\nConversions by channel (Last-Touch):\")\n",
    "print(conversions['touchpoint'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b798b0e",
   "metadata": {},
   "source": [
    "# Machine Learning Based MTA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b89ce24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bayesian Attribution:\n",
      "{'video': 0.17486857757843274, 'social': 0.18844845363975676, 'search': 0.17290025392415326, 'affiliate': 0.1617777383007844, 'display': 0.1542658800099353, 'email': 0.14773909654693762}\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import beta\n",
    "from scipy.special import expit\n",
    "\n",
    "def bayesian_attribution(df, n_iterations=1000):\n",
    "    touchpoints = df['touchpoint'].unique()\n",
    "    X = pd.get_dummies(df['touchpoint']).values\n",
    "    y = df['conversion'].values\n",
    "\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    beta_coeffs = np.zeros(n_features)\n",
    "    \n",
    "    # Prior parameters\n",
    "    alpha_prior = 1.0\n",
    "    beta_prior = 1.0\n",
    "    \n",
    "    # MCMC sampling\n",
    "    samples = np.zeros((n_iterations, n_features))\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Calculate probabilities\n",
    "        p = expit(X.dot(beta_coeffs))\n",
    "        \n",
    "        # Sample new coefficients\n",
    "        for j in range(n_features):\n",
    "            likelihood = np.prod(p[X[:, j] == 1] ** y[X[:, j] == 1] * (1 - p[X[:, j] == 1]) ** (1 - y[X[:, j] == 1]))\n",
    "            prior = beta.pdf(expit(beta_coeffs[j]), alpha_prior, beta_prior)\n",
    "            posterior = likelihood * prior\n",
    "            \n",
    "            # Metropolis-Hastings step\n",
    "            proposal = beta_coeffs[j] + np.random.normal(0, 0.1)\n",
    "            beta_coeffs_proposal = beta_coeffs.copy()\n",
    "            beta_coeffs_proposal[j] = proposal\n",
    "            p_proposal = expit(X.dot(beta_coeffs_proposal))\n",
    "            likelihood_proposal = np.prod(p_proposal[X[:, j] == 1] ** y[X[:, j] == 1] * (1 - p_proposal[X[:, j] == 1]) ** (1 - y[X[:, j] == 1]))\n",
    "            prior_proposal = beta.pdf(expit(proposal), alpha_prior, beta_prior)\n",
    "            posterior_proposal = likelihood_proposal * prior_proposal\n",
    "            \n",
    "            if np.random.random() < min(1, posterior_proposal / posterior):\n",
    "                beta_coeffs[j] = proposal\n",
    "        \n",
    "        samples[i] = beta_coeffs\n",
    "    \n",
    "    # Calculate attribution\n",
    "    attribution = {}\n",
    "    for i, channel in enumerate(touchpoints):\n",
    "        attribution[channel] = np.mean(samples[:, i])\n",
    "\n",
    "    # Normalize attribution\n",
    "    total = sum(attribution.values())\n",
    "    attribution = {channel: value / total for channel, value in attribution.items()}\n",
    "\n",
    "    return attribution\n",
    "\n",
    "print(\"\\nBayesian Attribution:\")\n",
    "print(bayesian_attribution(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a81eb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markov Chain Attribution:\n",
      "{'video': 0.16666666668406133, 'social': 0.16666666666493796, 'search': 0.16666666668301616, 'affiliate': 0.16666666663883137, 'display': 0.1666666666532864, 'email': 0.16666666667586677}\n"
     ]
    }
   ],
   "source": [
    "def markov_chain_attribution(df):\n",
    "    # Create transition matrix\n",
    "    touchpoints = df['touchpoint'].unique()\n",
    "    n = len(touchpoints)\n",
    "    transition_matrix = np.zeros((n+1, n+1))\n",
    "    \n",
    "    for _, user_data in df.groupby('user_id'):\n",
    "        touchpoint_sequence = user_data['touchpoint'].tolist()\n",
    "        for i in range(len(touchpoint_sequence)):\n",
    "            if i == len(touchpoint_sequence) - 1:\n",
    "                if user_data['conversion'].iloc[-1] == 1:\n",
    "                    transition_matrix[touchpoints.tolist().index(touchpoint_sequence[i])][n] += 1\n",
    "            else:\n",
    "                transition_matrix[touchpoints.tolist().index(touchpoint_sequence[i])][touchpoints.tolist().index(touchpoint_sequence[i+1])] += 1\n",
    "    \n",
    "    # Normalize transition matrix\n",
    "    row_sums = transition_matrix.sum(axis=1)\n",
    "    transition_matrix = np.divide(transition_matrix, row_sums[:, np.newaxis], where=row_sums[:, np.newaxis]!=0)\n",
    "    \n",
    "    # Calculate removal effect\n",
    "    base_conversion = transition_matrix[:n, n].sum()\n",
    "    attribution = {}\n",
    "    for i, channel in enumerate(touchpoints):\n",
    "        temp_matrix = transition_matrix.copy()\n",
    "        temp_matrix[i, :] = 0\n",
    "        temp_matrix[i, i] = 1\n",
    "        temp_conversion = np.linalg.matrix_power(temp_matrix, 100)[:n, n].sum()\n",
    "        attribution[channel] = base_conversion - temp_conversion\n",
    "    \n",
    "    # Normalize attribution\n",
    "    total_attribution = sum(attribution.values())\n",
    "    for channel in attribution:\n",
    "        attribution[channel] /= total_attribution\n",
    "    \n",
    "    return attribution\n",
    "\n",
    "print(\"Markov Chain Attribution:\")\n",
    "print(markov_chain_attribution(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15ab8546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph-Based (PageRank) Attribution:\n",
      "{'video': 0.17221980828053013, 'social': 0.16649305130743472, 'search': 0.17501687656568984, 'affiliate': 0.15545571884425657, 'display': 0.1624278933737435, 'email': 0.16838665162834526}\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def graph_attribution(df):\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    for _, user_data in df.groupby('user_id'):\n",
    "        touchpoints = user_data['touchpoint'].tolist()\n",
    "        for i in range(len(touchpoints) - 1):\n",
    "            if G.has_edge(touchpoints[i], touchpoints[i+1]):\n",
    "                G[touchpoints[i]][touchpoints[i+1]]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(touchpoints[i], touchpoints[i+1], weight=1)\n",
    "    \n",
    "    pagerank = nx.pagerank(G, weight='weight')\n",
    "    \n",
    "    # Normalize attribution\n",
    "    total = sum(pagerank.values())\n",
    "    attribution = {channel: pagerank.get(channel, 0) / total for channel in df['touchpoint'].unique()}\n",
    "    \n",
    "    return attribution\n",
    "\n",
    "print(\"\\nGraph-Based (PageRank) Attribution:\")\n",
    "print(graph_attribution(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a91521e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\n",
      "Deep Learning (RNN with Attention) Attribution:\n",
      "{'video': 0.10976741282191595, 'social': 0.047988214775738246, 'search': 0.06218911583340448, 'affiliate': 0.6177546376009803, 'display': 0.04049652473778955, 'email': 0.12180409423017147}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Attention, GlobalAveragePooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def create_rnn_attention_model(vocab_size, max_length):\n",
    "    inputs = Input(shape=(max_length,))\n",
    "    embedding = Embedding(vocab_size, 32)(inputs)\n",
    "    lstm = LSTM(64, return_sequences=True)(embedding)\n",
    "    attention = Attention()([lstm, lstm])\n",
    "    pooling = GlobalAveragePooling1D()(attention)\n",
    "    output = Dense(1, activation='sigmoid')(pooling)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Prepare data\n",
    "le = LabelEncoder()\n",
    "df['touchpoint_encoded'] = le.fit_transform(df['touchpoint'])\n",
    "\n",
    "max_length = df.groupby('user_id')['touchpoint'].count().max()\n",
    "\n",
    "X = df.groupby('user_id')['touchpoint_encoded'].apply(lambda x: x.tolist() + [0] * (max_length - len(x))).tolist()\n",
    "y = df.groupby('user_id')['conversion'].last().tolist()\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train model\n",
    "model = create_rnn_attention_model(len(le.classes_), max_length)\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Get attribution\n",
    "attribution = {}\n",
    "for channel in df['touchpoint'].unique():\n",
    "    channel_input = np.full((1, max_length), le.transform([channel])[0])\n",
    "    attribution[channel] = model.predict(channel_input)[0][0]\n",
    "\n",
    "# Normalize attribution\n",
    "total = sum(attribution.values())\n",
    "attribution = {channel: value / total for channel, value in attribution.items()}\n",
    "\n",
    "print(\"\\nDeep Learning (RNN with Attention) Attribution:\")\n",
    "print(attribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07f3d4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shapley Value Attribution:\n",
      "{'video': 0.24131463198474504, 'social': 0.2002106074213146, 'search': 0.18467668081806618, 'affiliate': 0.13679323568915447, 'display': 0.09147009914102003, 'email': 0.14553474494569962}\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def shapley_value_attribution(df):\n",
    "    touchpoints = df['touchpoint'].unique()\n",
    "    \n",
    "    def value(coalition):\n",
    "        if len(coalition) == 0:\n",
    "            return 0\n",
    "        mask = df['touchpoint'].isin(coalition)\n",
    "        return df.loc[mask, 'conversion'].sum() / df.loc[mask, 'user_id'].nunique()\n",
    "    \n",
    "    n = len(touchpoints)\n",
    "    attribution = {channel: 0 for channel in touchpoints}\n",
    "    \n",
    "    for channel in touchpoints:\n",
    "        for r in range(n):\n",
    "            subsets = combinations(set(touchpoints) - {channel}, r)\n",
    "            for subset in subsets:\n",
    "                coalition = set(subset)\n",
    "                margin = value(coalition | {channel}) - value(coalition)\n",
    "                attribution[channel] += margin / (n * len(list(combinations(touchpoints, r))))\n",
    "    \n",
    "    # Normalize attribution\n",
    "    total = sum(attribution.values())\n",
    "    attribution = {channel: value / total for channel, value in attribution.items()}\n",
    "    \n",
    "    return attribution\n",
    "\n",
    "print(\"\\nShapley Value Attribution:\")\n",
    "print(shapley_value_attribution(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcf3f8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Attribution:\n",
      "{'affiliate': 0.16194113131979124, 'display': 0.14408723096688394, 'email': 0.10748276195494735, 'search': 0.05636225186431357, 'social': 0.023559214269252563, 'video': 0.5065674096248114}\n",
      "Model Accuracy: 0.8850\n",
      "\n",
      "Gradient Boosting Machine Attribution:\n",
      "{'affiliate': 0.12338730105804087, 'display': 0.09719127796331732, 'email': 0.15222611068950972, 'search': 0.056922175908909, 'social': 0.01898387568561863, 'video': 0.5512892586946044}\n",
      "Model Accuracy: 0.8950\n",
      "\n",
      "Ensemble Attribution:\n",
      "{'affiliate': 0.14266421618891606, 'display': 0.12063925446510063, 'email': 0.12985443632222854, 'search': 0.05664221388661128, 'social': 0.021271544977435594, 'video': 0.528928334159708}\n",
      "Ensemble Model Accuracy: 0.8900\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Prepare the data\n",
    "le = LabelEncoder()\n",
    "df['touchpoint_encoded'] = le.fit_transform(df['touchpoint'])\n",
    "\n",
    "# Create features\n",
    "def create_features(group):\n",
    "    sequence = group['touchpoint_encoded'].tolist()\n",
    "    time_diff = (group['timestamp'].max() - group['timestamp'].min()).total_seconds() / 3600  # in hours\n",
    "    last_conversion = group['conversion'].iloc[-1]\n",
    "    return pd.Series({\n",
    "        'sequence': sequence,\n",
    "        'time_diff': time_diff,\n",
    "        'conversion': last_conversion\n",
    "    })\n",
    "\n",
    "features = df.groupby('user_id').apply(create_features)\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "max_length = features['sequence'].apply(len).max()\n",
    "X = np.array([seq + [0] * (max_length - len(seq)) for seq in features['sequence']])\n",
    "\n",
    "# Add time difference as a feature\n",
    "X = np.column_stack([X, features['time_diff'].values])\n",
    "\n",
    "y = features['conversion'].values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to calculate attribution\n",
    "def calculate_attribution(model, feature_importances):\n",
    "    attribution = {}\n",
    "    channel_importances = feature_importances[:len(le.classes_)]\n",
    "    for channel, importance in zip(le.classes_, channel_importances):\n",
    "        attribution[channel] = importance\n",
    "    \n",
    "    # Normalize attribution\n",
    "    total = sum(attribution.values())\n",
    "    attribution = {channel: value / total for channel, value in attribution.items()}\n",
    "    return attribution\n",
    "\n",
    "# Function to evaluate model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nRandom Forest Attribution:\")\n",
    "rf_attribution = calculate_attribution(rf_model, rf_model.feature_importances_)\n",
    "print(rf_attribution)\n",
    "evaluate_model(rf_model, X_test, y_test)\n",
    "\n",
    "# Gradient Boosting Machine\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nGradient Boosting Machine Attribution:\")\n",
    "gb_attribution = calculate_attribution(gb_model, gb_model.feature_importances_)\n",
    "print(gb_attribution)\n",
    "evaluate_model(gb_model, X_test, y_test)\n",
    "\n",
    "# Ensemble (Simple average of attributions)\n",
    "print(\"\\nEnsemble Attribution:\")\n",
    "ensemble_attribution = {}\n",
    "for channel in le.classes_:\n",
    "    ensemble_attribution[channel] = (rf_attribution[channel] + gb_attribution[channel]) / 2\n",
    "\n",
    "# Normalize ensemble attribution\n",
    "total = sum(ensemble_attribution.values())\n",
    "ensemble_attribution = {channel: value / total for channel, value in ensemble_attribution.items()}\n",
    "print(ensemble_attribution)\n",
    "\n",
    "# Evaluate ensemble (using simple voting)\n",
    "ensemble_predictions = (rf_model.predict_proba(X_test)[:, 1] + gb_model.predict_proba(X_test)[:, 1]) / 2\n",
    "ensemble_predictions = (ensemble_predictions > 0.5).astype(int)\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "print(f\"Ensemble Model Accuracy: {ensemble_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9046baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
      "https://github.com/hmmlearn/hmmlearn/issues/335\n",
      "https://github.com/hmmlearn/hmmlearn/issues/340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points: 3024\n",
      "\n",
      "Hidden Markov Model Attribution:\n",
      "Number of unique touchpoints: 6\n",
      "Shape of X: (3024, 1)\n",
      "Shape of emission_matrix: (3, 1)\n",
      "Warning: Touchpoint display (index 1) not covered by emission matrix\n",
      "Warning: Touchpoint email (index 2) not covered by emission matrix\n",
      "Warning: Touchpoint search (index 3) not covered by emission matrix\n",
      "Warning: Touchpoint social (index 4) not covered by emission matrix\n",
      "Warning: Touchpoint video (index 5) not covered by emission matrix\n",
      "{'affiliate': nan, 'display': nan, 'email': nan, 'search': nan, 'social': nan, 'video': nan}\n",
      "\n",
      "Latent Dirichlet Allocation Attribution:\n",
      "{'affiliate': 0.1434880786701127, 'display': 0.2003766555038235, 'email': 0.16665839337524785, 'search': 0.13999817723706937, 'social': 0.18400263317100113, 'video': 0.1654760620427454}\n",
      "\n",
      "Comparison of Attributions:\n",
      "Channel    HMM        LDA       \n",
      "------------------------------\n",
      "affiliate  nan        0.1435    \n",
      "display    nan        0.2004    \n",
      "search     nan        0.1400    \n",
      "social     nan        0.1840    \n",
      "video      nan        0.1655    \n",
      "email      nan        0.1667    \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from hmmlearn import hmm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "print(f\"Total number of data points: {len(df)}\")\n",
    "\n",
    "# Prepare the data\n",
    "le = LabelEncoder()\n",
    "df['touchpoint_encoded'] = le.fit_transform(df['touchpoint'])\n",
    "\n",
    "# 1. Hidden Markov Model (HMM)\n",
    "print(\"\\nHidden Markov Model Attribution:\")\n",
    "\n",
    "# Prepare sequences for HMM\n",
    "sequences = df.groupby('user_id')['touchpoint_encoded'].apply(list).tolist()\n",
    "lengths = [len(seq) for seq in sequences]\n",
    "X = np.concatenate(sequences).reshape(-1, 1)\n",
    "\n",
    "# Print diagnostic information\n",
    "print(f\"Number of unique touchpoints: {len(le.classes_)}\")\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "\n",
    "# Train HMM\n",
    "n_components = min(3, len(le.classes_))  # Ensure n_components is not greater than number of touchpoints\n",
    "model = hmm.MultinomialHMM(n_components=n_components, n_iter=100, random_state=42)\n",
    "model.fit(X, lengths)\n",
    "\n",
    "# Calculate attribution\n",
    "transition_matrix = model.transmat_\n",
    "emission_matrix = model.emissionprob_\n",
    "\n",
    "print(f\"Shape of emission_matrix: {emission_matrix.shape}\")\n",
    "\n",
    "hmm_attribution = {}\n",
    "for i, touchpoint in enumerate(le.classes_):\n",
    "    if i < emission_matrix.shape[1]:\n",
    "        importance = np.mean(emission_matrix[:, i])\n",
    "        hmm_attribution[touchpoint] = importance\n",
    "    else:\n",
    "        print(f\"Warning: Touchpoint {touchpoint} (index {i}) not covered by emission matrix\")\n",
    "        hmm_attribution[touchpoint] = 0\n",
    "\n",
    "\n",
    "\n",
    "# Normalize attribution\n",
    "total = sum(hmm_attribution.values())\n",
    "hmm_attribution = {channel: value / total for channel, value in hmm_attribution.items()}\n",
    "print(hmm_attribution)\n",
    "\n",
    "# 2. Latent Dirichlet Allocation (LDA) using scikit-learn\n",
    "print(\"\\nLatent Dirichlet Allocation Attribution:\")\n",
    "\n",
    "# Prepare documents for LDA\n",
    "documents = df.groupby('user_id')['touchpoint'].apply(lambda x: ' '.join(x)).tolist()\n",
    "\n",
    "# Create document-term matrix\n",
    "vectorizer = CountVectorizer()\n",
    "doc_term_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda_output = lda_model.fit_transform(doc_term_matrix)\n",
    "\n",
    "# Calculate attribution\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "topic_term_dist = lda_model.components_ / lda_model.components_.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "lda_attribution = defaultdict(float)\n",
    "for topic_idx, topic in enumerate(topic_term_dist):\n",
    "    for i, value in enumerate(topic):\n",
    "        lda_attribution[feature_names[i]] += value\n",
    "\n",
    "# Normalize attribution\n",
    "total = sum(lda_attribution.values())\n",
    "lda_attribution = {channel: value / total for channel, value in lda_attribution.items()}\n",
    "print(dict(lda_attribution))\n",
    "\n",
    "# Comparison of attributions\n",
    "print(\"\\nComparison of Attributions:\")\n",
    "all_channels = set(hmm_attribution.keys()) | set(lda_attribution.keys())\n",
    "\n",
    "print(f\"{'Channel':<10} {'HMM':<10} {'LDA':<10}\")\n",
    "print(\"-\" * 30)\n",
    "for channel in all_channels:\n",
    "    hmm_value = hmm_attribution.get(channel, 0)\n",
    "    lda_value = lda_attribution.get(channel, 0)\n",
    "    print(f\"{channel:<10} {hmm_value:<10.4f} {lda_value:<10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2693c2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simplified Causal Forest Attribution:\n",
      "{'affiliate': 0.16666666666666663, 'display': 0.16666666666666663, 'email': 0.16666666666666663, 'search': 0.16666666666666663, 'social': 0.16666666666666663, 'video': 0.16666666666666663}\n",
      "\n",
      "Simplified Double Machine Learning Attribution:\n",
      "{'affiliate': 0.16666666666666663, 'display': 0.16666666666666663, 'email': 0.16666666666666663, 'search': 0.16666666666666663, 'social': 0.16666666666666663, 'video': 0.16666666666666663}\n",
      "\n",
      "Comparison of Attributions:\n",
      "Channel    Causal Forest   Double ML      \n",
      "----------------------------------------\n",
      "affiliate  0.1667          0.1667         \n",
      "display    0.1667          0.1667         \n",
      "search     0.1667          0.1667         \n",
      "social     0.1667          0.1667         \n",
      "video      0.1667          0.1667         \n",
      "email      0.1667          0.1667         \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Prepare the data\n",
    "le = LabelEncoder()\n",
    "df['touchpoint_encoded'] = le.fit_transform(df['touchpoint'])\n",
    "\n",
    "# Create features\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "X = ohe.fit_transform(df[['touchpoint']])\n",
    "feature_names = ohe.get_feature_names_out(['touchpoint'])\n",
    "\n",
    "# Create treatment and outcome variables\n",
    "T = df['touchpoint_encoded'].values\n",
    "y = df['conversion'].values\n",
    "\n",
    "# Create some additional features\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "W = df[['day_of_week', 'month']].values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, T_train, T_test, y_train, y_test, W_train, W_test = train_test_split(X, T, y, W, test_size=0.2, random_state=42)\n",
    "\n",
    "# 1. Simplified Causal Forest\n",
    "print(\"\\nSimplified Causal Forest Attribution:\")\n",
    "\n",
    "def simplified_causal_forest(X, T, y, W):\n",
    "    # Train a random forest to predict treatment\n",
    "    rf_t = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "    rf_t.fit(W, T)\n",
    "    \n",
    "    # Get residuals for treatment\n",
    "    T_pred = rf_t.predict(W)\n",
    "    T_residual = T - T_pred\n",
    "    \n",
    "    # Train a random forest to predict outcome\n",
    "    rf_y = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "    rf_y.fit(W, y)\n",
    "    \n",
    "    # Get residuals for outcome\n",
    "    y_pred = rf_y.predict(W)\n",
    "    y_residual = y - y_pred\n",
    "    \n",
    "    # Final stage: regress outcome residuals on treatment residuals\n",
    "    final_model = LinearRegression()\n",
    "    final_model.fit(T_residual.reshape(-1, 1), y_residual)\n",
    "    \n",
    "    return final_model.coef_[0]\n",
    "\n",
    "cf_effects = simplified_causal_forest(X_train, T_train, y_train, W_train)\n",
    "\n",
    "cf_attribution = {}\n",
    "for i, touchpoint in enumerate(le.classes_):\n",
    "    cf_attribution[touchpoint] = abs(cf_effects)\n",
    "\n",
    "# Normalize attribution\n",
    "total = sum(cf_attribution.values())\n",
    "cf_attribution = {channel: value / total for channel, value in cf_attribution.items()}\n",
    "print(cf_attribution)\n",
    "\n",
    "# 2. Simplified Double Machine Learning\n",
    "print(\"\\nSimplified Double Machine Learning Attribution:\")\n",
    "\n",
    "def simplified_double_ml(X, T, y, W):\n",
    "    # First stage: predict T using W\n",
    "    model_t = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "    model_t.fit(W, T)\n",
    "    T_pred = model_t.predict(W)\n",
    "    T_residual = T - T_pred\n",
    "\n",
    "    # First stage: predict y using W\n",
    "    model_y = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "    model_y.fit(W, y)\n",
    "    y_pred = model_y.predict(W)\n",
    "    y_residual = y - y_pred\n",
    "\n",
    "    # Second stage: regress y_residual on T_residual\n",
    "    final_model = LinearRegression()\n",
    "    final_model.fit(T_residual.reshape(-1, 1), y_residual)\n",
    "    \n",
    "    return final_model.coef_[0]\n",
    "\n",
    "dml_effects = simplified_double_ml(X_train, T_train, y_train, W_train)\n",
    "\n",
    "dml_attribution = {}\n",
    "for i, touchpoint in enumerate(le.classes_):\n",
    "    dml_attribution[touchpoint] = abs(dml_effects)\n",
    "\n",
    "# Normalize attribution\n",
    "total = sum(dml_attribution.values())\n",
    "dml_attribution = {channel: value / total for channel, value in dml_attribution.items()}\n",
    "print(dml_attribution)\n",
    "\n",
    "# Comparison of attributions\n",
    "print(\"\\nComparison of Attributions:\")\n",
    "all_channels = set(cf_attribution.keys()) | set(dml_attribution.keys())\n",
    "\n",
    "print(f\"{'Channel':<10} {'Causal Forest':<15} {'Double ML':<15}\")\n",
    "print(\"-\" * 40)\n",
    "for channel in all_channels:\n",
    "    cf_value = cf_attribution.get(channel, 0)\n",
    "    dml_value = dml_attribution.get(channel, 0)\n",
    "    print(f\"{channel:<10} {cf_value:<15.4f} {dml_value:<15.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
